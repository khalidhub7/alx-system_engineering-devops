ğŸ‚ Postmortem ğŸ‚
Postmortem Report: Web Stack Outage ğŸš¨
____________________________________________________________________

Issue Summary
Duration of Outage: June 20, 2024, 10:00 AM - 12:30 PM (PST) â°

Impact: Our main shopping site was down, stopping users from making purchases. About 75% of users were affected. ğŸ˜±

Root Cause: A wrong setting in the database connection pool caused the database to overload, leading to a system crash. ğŸ› ï¸

![wallpaperflare com_wallpaper](https://github.com/khalidhub7/alx-system_engineering-devops/assets/139714446/fed6cf47-dfbb-4cb9-a96c-8b8399075714)

Timeline
10:00 AM: Issue detected by monitoring system showing high error rates. ğŸš¨
10:05 AM: Engineering team notified via PagerDuty. ğŸ“Ÿ
10:10 AM: Initial investigation focused on web server, assuming traffic spike. ğŸŒ
10:30 AM: Realized web server was fine, switched focus to the database. ğŸ”
10:45 AM: Checked database logs, saw high connection counts. ğŸ“Š
11:00 AM: Suspected database connection pool misconfiguration. â“
11:15 AM: Misleading check for potential DDoS attack found nothing. ğŸ•µï¸
11:30 AM: Escalated to senior database admin. ğŸ“
11:45 AM: Fixed connection pool settings. âœ…
12:00 PM: Restarted database, monitored connections. ğŸ”„
12:30 PM: Service fully restored. ğŸ‰

Root Cause and Resolution
Root Cause: The database connection pool was set up wrong, allowing too few connections. This caused the database to crash under heavy use. ğŸ˜“

Resolution: We fixed the issue by increasing the maximum number of database connections. After changing the settings, we restarted the database, and everything went back to normal. ğŸ”§

Corrective and Preventative Measures
Improvements/Fixes:

Improve how we manage database settings. ğŸ“‹
Better monitoring to catch connection issues early. ğŸš€
Regular load tests to ensure settings handle peak times. ğŸ§ª

![wallpaLLperflare com_wallpaper](https://github.com/khalidhub7/alx-system_engineering-devops/assets/139714446/f5d027ad-fae2-4975-adf4-c9e9fe8d8543)

TODO List:

Fix Database Connection Settings:
Review and adjust connection limits. ğŸ› ï¸
Improve Monitoring:
Add alerts for database connection issues. ğŸ””
Conduct Load Testing:
Schedule regular tests to check connection settings. ğŸ“ˆ
Update Documentation:
Document the correct settings and procedures. ğŸ“š
Training:
Train the team on managing database connections and using monitoring tools. ğŸ“
Conclusion
By fixing the root cause and improving our processes, we aim to prevent this from happening again. This postmortem shows the importance of good configuration and monitoring. Thanks to the team for quickly solving the problem. We'll keep working to improve our service. ğŸŒŸ
